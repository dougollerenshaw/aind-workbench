[
  {
    "platform": "Behavior",
    "lead": "Arielle",
    "title": "VR Foraging processed QC and NWB packaging capsule",
    "priority": "P0",
    "size": "medium",
    "description": "This capsule should use the rig.json to identify which Harp registers/lines are active during a VR Foraging session. The capsule will be configured to read only these active lines from the raw NWB and determine if they are events or continuous data. The event signals will be documented in the processed event table. QC plots will need to be identified from the event data as well as any QC defined by the user.",
    "status": "The work is a carryover from Q2 and is in progress but not complete.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 20%"
  },
  {
    "platform": "Behavior",
    "lead": "Arielle",
    "title": "Combine existing VR Foraging capsules into a full production pipeline",
    "priority": "P0",
    "size": "small",
    "description": "Combine the raw and processing VR Foraging NWB packaging capsules into a pipeline which will output a final NWB plus relevant metadata.",
    "status": "Not started.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 5%"
  },
  {
    "platform": "Behavior",
    "lead": "Arielle",
    "title": "Create behavior pipeline template",
    "priority": "P2",
    "size": "small",
    "description": "We plan to build a generic template of a processing pipeline to help users to build their own pipeline. This should help reduce reliance on scientific computing for future behavior projects.",
    "status": "Not started.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 5%"
  },
  {
    "platform": "Behavior",
    "lead": "Arielle",
    "title": "Upgrade metadata to schema 2.0",
    "priority": "P0",
    "size": "medium",
    "description": "\u2022 Identify all instruments using physio-behavior processing pipelines \n\u2022 Update pipelines to be backwards compatible with 1.x and 2.0 schema versions \n\u2022 Update metadata mapper code \n\u2022 Coordinate with users to get code deployed on rigs (make an excel sheet to track what's been done and what's been deployed) \n\u2022 Do we need to help with rig.jsons?",
    "status": "Not started.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 15%, Ahad: 15%"
  },
  {
    "platform": "Behavior",
    "lead": "Arielle",
    "title": "BCI behavior wrap-up",
    "priority": "P0",
    "size": "medium",
    "description": "Finish work related to the BCI behavior task. We made substantial progress in cleaning up the data processing pipeline during preparations for the May hackathon at UW. It\u2019s important to maintain this momentum to ensure that these efforts are translated into a robust processing pipeline.",
    "status": "This is a continuation of work begun in the leadup to the May UW Hackathon event.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 20%"
  },
  {
    "platform": "Behavior",
    "lead": "Arielle",
    "title": "Ongoing support (solving github issues)",
    "priority": "P1",
    "size": "medium",
    "description": "Provide ongoing support for existing pipelines. This involves triaging and solving issues submitted by scientists through Github.",
    "status": "Not started.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 20%"
  },
  {
    "platform": "Analysis",
    "lead": "Arielle",
    "title": "NWB API for multi-modal analysis",
    "priority": "P0",
    "size": "medium",
    "description": "Build a tool that will allow users who are running session level analysis to interact with distinct, multi-modal NWBs as if they were interacting with one. The tool will need to read in the distinct NWBs and return a single NWB object for the user to interact with.",
    "status": "There is no way for users to run session level analysis with distinct, multi-modal NWBs.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 20%"
  },
  {
    "platform": "Analysis",
    "lead": "Arielle",
    "title": "Scaled analysis",
    "priority": "P0",
    "size": "small",
    "description": "The analysis pipeline needs to support multiple analysis per derived asset. We will scale from N jobs (current system) to N x M jobs, where N = derived asset and M = analysis type",
    "status": "Job dispatch and analysis wrapper templates exist as well as a pipeline but do not contain instructions on how to work with the pipeline. The goal is to create a low-friction user experience for running scaled, versioned analysis.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 6%"
  },
  {
    "platform": "Analysis",
    "lead": "Arielle",
    "title": "Add nested outputs to job-dispatch",
    "priority": "P0",
    "size": "small",
    "description": "Users need the ability to pass nested, or grouped, asset ids to their analysis wrapper. For example, a query should output groups of assets that can be passed to a given analysis capsule.",
    "status": "Does not exist.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 6%"
  },
  {
    "platform": "Analysis",
    "lead": "Arielle",
    "title": "Viewing application of analysis results",
    "priority": "P1",
    "size": "medium",
    "description": "Users need to check the results of their analysis after running the analysis pipeline. This can be achieved by building a Panel application that can query analysis location information from analysis collections in the document database.",
    "status": "Does not exist.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 20%"
  },
  {
    "platform": "Analysis",
    "lead": "Arielle",
    "title": "Documentation and templates",
    "priority": "P0",
    "size": "medium",
    "description": "Publish documentation with clear examples on how to use the analysis pipeline. The goal is to create a template pipeline with clear instructions that would be frictionless for any non-power user to replicate.",
    "status": "Working on finalizing templates for both the job dispatch and analysis wrapper",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arielle: 10%"
  },
  {
    "platform": "Analysis",
    "lead": "Arielle",
    "title": "Continuous analysis pipeline",
    "priority": "P1",
    "size": "medium",
    "description": "For analysis that needs to be run on all derived assets output from a processing pipeline, a system is required that can check for new assets as they are processed. The trigger system will need access to the results collection so that it can check if analysis has already been run on a given asset.",
    "status": "Does not exist.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 10%"
  },
  {
    "platform": "Fiber Photometry",
    "lead": "Arielle",
    "title": "Release fiber-only pipeline",
    "priority": "P0",
    "size": "small",
    "description": "The fiber pipeline has been built - but it needs to check off the steps to make it officially complete. To be complete, it must contain all valid metadata, move it to main and build and official CO release.",
    "status": "It has been built. Just needs final touches",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 6%"
  },
  {
    "platform": "Fiber Photometry",
    "lead": "Arielle",
    "title": "Backlog re-processing",
    "priority": "P0",
    "size": "tiny",
    "description": "Reprocess backlog tickets.",
    "status": "Not started",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 2%"
  },
  {
    "platform": "Fiber Photometry",
    "lead": "Arielle",
    "title": "Upgrade metadata to schema 2.0",
    "priority": "P0",
    "size": "medium",
    "description": "\u2022 Identify all instruments using physio-behavior processing pipelines \n\u2022 Update pipelines to be backwards compatible with 1.x and 2.0 schema versions \n\u2022 Update metadata mapper code \n\u2022 Coordinate with users to get code deployed on rigs (make an excel sheet to track what's been done and what's been deployed) \n\u2022 Do we need to help with rig.jsons?",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 15%, Ahad: 15%"
  },
  {
    "platform": "Fiber Photometry",
    "lead": "Arielle",
    "title": "Convert rigs to upload using modular pipelines",
    "priority": "P1",
    "size": "medium",
    "description": "Coordinate with users to submit jobs to the aind-data-transfer service V2 and point to the fiber specific job - type that will process through both fiber and behavior pipelines",
    "status": "Not started",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 20%"
  },
  {
    "platform": "Fiber Photometry",
    "lead": "Arielle",
    "title": "Ongoing support (solving github issues)",
    "priority": "P1",
    "size": "medium",
    "description": "Provide ongoing support for existing pipelines. This involves triaging and solving issues submitted by scientists through Github.",
    "status": "Not started.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 20%"
  },
  {
    "platform": "Dataverse - PowerApps",
    "lead": "Arielle",
    "title": "Data tables in Dataverse for Waterlog",
    "priority": "P0",
    "size": "small",
    "description": "Implementation and validation of tables in the AIBS Dataverse environment. The waterlog application will post data to Dataverse and LIMS (initially).",
    "status": "Architectual work as well as some data tables have been made.",
    "success_criteria": "Write data to Dataverse successfully from the waterlog application.",
    "risks": null,
    "resource_summary": "Clark: 3%"
  },
  {
    "platform": "Dataverse - PowerApps",
    "lead": "Arielle",
    "title": "Two-way integration and restriction tracking",
    "priority": "P0",
    "size": "medium",
    "description": "Waterlog can read and write to Dataverse. More tables will need to be added for state changes (water on-off) for mice in active behavior.",
    "status": "Not done.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Clark: 10%"
  },
  {
    "platform": "Dataverse - PowerApps",
    "lead": "Arielle",
    "title": "Behavior protocol integration and automation",
    "priority": "P1",
    "size": "medium",
    "description": "Application viewer for users to determine where the mouse is in its behavior protocol. Model full behavioral training progression and enable automated transitions and calculations as extra validation for water restricting mice.",
    "status": "Not done.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Clark: 30%"
  },
  {
    "platform": "Dataverse - PowerApps",
    "lead": "Arielle",
    "title": "Inventory system",
    "priority": "P1",
    "size": "Medium/Large",
    "description": "Create types of locations (freezer, bench top, etc.), types of inventories (reagent, dyes, viruses, etc.) and their supporting information, status of items (empty, full, etc.) and add logic around how it shows up in inventory. Export data in SLIMS and upload to Dataverse. Requires barcoding, build flows and create forms for users to view / update items in system.",
    "status": "Not done.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Clark: 40%"
  },
  {
    "platform": "Dataverse - PowerApps",
    "lead": "Arielle",
    "title": "Ongoing support for transition from SLIMS to Dataverse",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "Supporting users in SLIMS while the transition is ongoing into Dataverse. This includes but is not limited to exporting data from SLIMS to Dataverse. Maintaining current workflows and remap data to the Dataverse system.",
    "status": "Not done.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Clark: 40%"
  },
  {
    "platform": "2P Ophys",
    "lead": "Arielle",
    "title": "Cell matching evaluation in qc portal",
    "priority": "P0",
    "size": "small",
    "description": "For all child sessions acquired on the mesoscope, we need an evaluation that will reference the parent session images for cell matching. DocDB should be queried to look for earliest creation date with passed QC.",
    "status": "Not started",
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "2P Ophys",
    "lead": "Arielle",
    "title": "Create ISI metadata and ISI capsule",
    "priority": "P0",
    "size": "small",
    "description": "David, Di will build the ISI capsule and Arielle will attach metadata to it",
    "status": "Finished",
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "2P Ophys",
    "lead": "Arielle",
    "title": "Test and point rigs to upload to the new pipeline",
    "priority": "P0",
    "size": "medium",
    "description": "Coordinate with users to submit jobs to the aind-data-transfer service V2 and point to the fiber specific job - type that will process through both ophys and behavior pipelines",
    "status": "Not started",
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "2P Ophys",
    "lead": "Arielle",
    "title": "Data and metadata cleanup",
    "priority": "P0",
    "size": "medium",
    "description": "Fix derived data descriptions, session metadata and any other metadata that needs to be updated. Remove duplicate sessions.",
    "status": "Not started",
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "2P Ophys",
    "lead": "Arielle",
    "title": "Ongoing support (supporting releases and summer workshop)",
    "priority": "P0",
    "size": "medium",
    "description": "Provide ongoing support for existing pipelines. This involves triaging and solving issues submitted by scientists through Github.",
    "status": "Not started",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arielle: 20%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "SWDB",
    "priority": "P0",
    "size": "Large",
    "description": "Prepare and run the Summer Workshop. Preparations are currently underway for the course that is being held August 24-Sept 7th. Key activities include: \n* Prepare V1DD data. We are replacing MICrONS data with the V1DD data for the first time. This data also needs to be uploaded to DANDI in support of two preprint/publications using it. \n* Prepare NP Ultra data. Finalize NWB files and metadata. \n* Prepare Dynamic Foraging data. \n* Clean up BCI. \n* Update DataBook for each of these.",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Saskia: 60%, Dan: 20%, Suyee: 40%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Manuscript on Chatbot",
    "priority": "P0",
    "size": "Large",
    "description": "Write a manuscript on the metadata chatbot that describes the tools and the benchmarking results.",
    "status": null,
    "success_criteria": "Draft of a manuscript on chatbot and benchmarking",
    "risks": null,
    "resource_summary": "Saskia: 10%, Sreya: 70%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Continued development of Chatbot",
    "priority": "P1",
    "size": "Medium",
    "description": "Continue to improve chatbot and mcp performance",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Sreya: 30%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "AIND Data Schema v2.0 deployment",
    "priority": "P0",
    "size": "Large",
    "description": "V2.0 is being released this week. The upgrader is working on upgrading existing 1.0 records, and identifying migration issues in its wake.",
    "status": null,
    "success_criteria": "* Upgrade existing records (80% success?) \n* Release tutorial on creating metadata  \n* Metadata mappers updated \n* Update QC and metadata portals to use v2",
    "risks": null,
    "resource_summary": "Doug: 10%, Dan: 80%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Analysis Architecture",
    "priority": "P0",
    "size": "Medium",
    "description": null,
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Support external users",
    "priority": "P1",
    "size": "Medium",
    "description": "Help Templeton collaborators get data and metadata into the platform",
    "status": null,
    "success_criteria": "External data successfully on platform and a system for ongoing upload is in place",
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "LC Paper publication",
    "priority": "P0",
    "size": "Large",
    "description": "v2.0 is being released this week. The upgrader is working on upgrading existing 1.0 records, and identifying migration issues in its wake.",
    "status": null,
    "success_criteria": "* Sue\u2019s metadata created for historic data \n* Clean up existing records \n* Support new sequencing modalities (stretch) \n* Morphology analysis \n* Create public collection of assets and capsules",
    "risks": null,
    "resource_summary": "Doug: 60%, Tom: 20%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Data Challenge",
    "priority": "P0",
    "size": "Medium",
    "description": "Preparation for a data challenge on image compression to be held in early 2026. Kaggle research application is due August 11th.",
    "status": null,
    "success_criteria": "* Define data for data challenge.  \n* Define metrics of evaluation \n* Submit application to Kaggle (deadline August 11th ).",
    "risks": null,
    "resource_summary": "Suyee: 30%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Move Dynamic Routing data into the public bucket",
    "priority": "P0",
    "size": "Large",
    "description": "Much of our data is still being put into private buckets. We are working to move assets into the open bucket in compliance with our policy and to facilitate making public collections. This involves fixing a lot of missing metadata, so it can\u2019t be completed in one shot. We have created a dashboard to track data in different buckets according to project.",
    "status": null,
    "success_criteria": "Move all of the data for \u201cDynamic Routing\u201d to the open data bucket to support upcoming paper",
    "risks": null,
    "resource_summary": "Doug: 30%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Other summer workshops",
    "priority": "P1",
    "size": "Small",
    "description": "Participating in TReND (remotely), the Coding Workshop, the Technical Workshop.",
    "status": null,
    "success_criteria": "Finalize presentations",
    "risks": null,
    "resource_summary": "Saskia: 10%, Suyee: 10%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Fall workshop planning",
    "priority": "P1",
    "size": "Small",
    "description": "We are planning a UW CNC hackathon in the fall as well as a workshop at WWU. These will be in Q4, but planning will need to happen before then. The WWU should largely leverage existing content. The UW event will likely involve some preparation for hackathon notebooks, etc. Ideally this will leverage content from SWDB.",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Suyee: 20%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "protocol and website",
    "priority": "P2",
    "size": "Small",
    "description": null,
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Platform Paper",
    "priority": "P1",
    "size": "Large",
    "description": "Write a manuscript on the data platform strategy that highlights our infrastructure and philosophy.",
    "status": null,
    "success_criteria": "Full draft ready for submission",
    "risks": null,
    "resource_summary": "Saskia: 30%"
  },
  {
    "platform": "Ephys",
    "lead": "David",
    "title": "Reliable chronic electrophysiology uploads",
    "priority": "P0",
    "size": "Small",
    "description": "Finalize and test the chronic ephys chunked uploads and compression job.",
    "status": "Testing",
    "success_criteria": "Production run succeeds without issue.",
    "risks": "Coordination with SciComp",
    "resource_summary": "Alessio: 20%"
  },
  {
    "platform": "Ephys",
    "lead": "David",
    "title": "Chronic Spike Sorting",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "Develop sorting workflow that is compatible with chronic recordings",
    "status": "Not started",
    "success_criteria": "Production data set sorts successfully",
    "risks": "Depends on DartSort developers delivering DartSort.",
    "resource_summary": null
  },
  {
    "platform": "Ephys",
    "lead": "David",
    "title": "Ephys pipeline manuscript",
    "priority": "P0",
    "size": "Large",
    "description": "Write and submit a manuscript describing the ephys pipeline.",
    "status": "In development",
    "success_criteria": "Preprint on biorxiv",
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Ephys",
    "lead": "David",
    "title": "SpikeInterface GUI",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "Fully integrate SpikeInterface GUI into QC portal",
    "status": "In development",
    "success_criteria": "Users are using SIGUI inside of QC portal",
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Slap2",
    "lead": "David",
    "title": "Evaluate parameters and algorithms for motion correction and trace extraction",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "Evaluate parameters and algorithms for motion correction and trace extraction.",
    "status": "Almost finished",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Johannes: 30%"
  },
  {
    "platform": "Slap2",
    "lead": "David",
    "title": "Implement SLAP2 processing pipeline",
    "priority": "P1",
    "size": "Medium/Large",
    "description": "Implement SLAP2 pipeline based on evaluated parameters.",
    "status": "Not started",
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "SmartSPIM",
    "lead": "Sharmi",
    "title": "Setup SmartSPIM Pipeline on Slurm Cluster for routine Processing",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "Currently the pipeline is only deployed on CO. However, we need to be able to sometimes reprocess datasets and it would be valuable to be able to do that at a lower cost on the Slurm cluster. This work involves testing this functionality.",
    "status": "This is underway",
    "success_criteria": "Running 3-5 datasets through the slurm cluster and QC'ing results",
    "risks": "Scientists for QC, Slurm Cluster",
    "resource_summary": "Camilo: 15%"
  },
  {
    "platform": "SmartSPIM",
    "lead": "Sharmi",
    "title": "Upgrade SmartSPIM Pipeline to incorporate new spot detection and new classification model",
    "priority": "P0",
    "size": "medium/large",
    "description": "A new spot detection and classification model has been developed and needs to be QC'ed and deployed into the pipeline",
    "status": "Currently blocked by our ability to run on the Slurm Cluster where we are testing the new detection model",
    "success_criteria": "Running 5 datasets and QC'ing them with scientists",
    "risks": "Scientists for QC, CO",
    "resource_summary": "Camilo: 20%"
  },
  {
    "platform": "SmartSPIM",
    "lead": "Sharmi",
    "title": "Test External DataSet on Slurm Cluster",
    "priority": "P1",
    "size": "small/medium",
    "description": "UW submitted a brain to us and we need to run it through our pipeline to show it's impact for external labs (and our paper)",
    "status": "Partially run",
    "success_criteria": "An automated run of the UW dataset with results that are QC'ed by the UW group",
    "risks": "QC from UW, slurm cluster, updates needed for adapting to differences in the data",
    "resource_summary": "Camilo: 15%"
  },
  {
    "platform": "ExaSPIM",
    "lead": "Sharmi",
    "title": "Set up automated pipeline for uploads, flatfielding and alignment",
    "priority": "P0",
    "size": "Small",
    "description": "The current pipeline does not automatically trigger all of these steps and required human communication. This will be automated.",
    "status": "Pieces are complete and need to be connected and tested",
    "success_criteria": "3-5 datasets running through the pipeline in an automated manner and generating an alignment result.",
    "risks": "Code Ocean Failures, aind-transfer-service",
    "resource_summary": "Di: 10%"
  },
  {
    "platform": "ExaSPIM",
    "lead": "Sharmi",
    "title": "Setup pipeline for CCF Registration, Soma detection and quantification",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "The different capsules have been setup for various processing, but we need to do some infrastructure work to connect them and ensure seamless outputs",
    "status": "Each piece is complete but lots of inconsistency in data outputs",
    "success_criteria": "3-5 datasets running through the pipeline in an automated manner",
    "risks": "This will be setup as two pipelines since we are still debugging the latest bigstitcher for fusing the signal channel. Upgrades to capsules can delay this",
    "resource_summary": "Di: 40%"
  },
  {
    "platform": "ExaSPIM",
    "lead": "Sharmi",
    "title": "Develop metrics for comparing across alignment results",
    "priority": "p0",
    "size": "Large",
    "description": "Currently we are unable to compare registration of the same interest points when the alignment changes since bigstitcher only outputs collated summary statistics. We will with with the Octo (Rhapso Team) to develop metrics that will enable this.",
    "status": "Not started.",
    "success_criteria": "Code that will output a value for every interest point \n\nCode that allows a user to select to interest points and transformations to compute metrics",
    "risks": "Changes to bigstitcher outputs and performance can delay this, A QC loop with scientists is also needed",
    "resource_summary": "Di: 50%"
  },
  {
    "platform": "ExaSPIM",
    "lead": "Sharmi",
    "title": "Image compression",
    "priority": "p0",
    "size": "Large",
    "description": "Currently we compress with standard zstd libraries. However, we can optimize more for our data. This work is an exploration of the different methods for compression.",
    "status": "Ongoing",
    "success_criteria": "Identifying an alternative compression method that we can deploy into our pipeline or reasons why the current one is the best.",
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Z1",
    "lead": "Sharmi",
    "title": "Setup Automated Pipeline For uploads, radial correction, stitching",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "This work involves setting up of a common pipeline for processing all datasets up to stitching that are acquired on the Z1: Multitile and Single Tile HCR and Proteomics Data",
    "status": "Implementation of uploads is complete",
    "success_criteria": "An automated pipeline that includes: uploads, radial correction, camera correction and stitching, where each step has been verified by scientists.",
    "risks": "CO upgrades, aind-data-transfer-service, QC from scientists",
    "resource_summary": "Carson: 100%"
  },
  {
    "platform": "Z1",
    "lead": "Sharmi",
    "title": "Determine an optimal method for computing R2R registration",
    "priority": "P0",
    "size": "Large",
    "description": "R2R registration with bigstream has proved to be too expensive to deploy for routine data processing. An alternative method needs to be developed.",
    "status": "Tests with Bigstitcher are promising",
    "success_criteria": "A R2R registration method that can register two rounds within $200 of compute costs",
    "risks": "Bigstitcher, Bigstream",
    "resource_summary": "Sean: 100%"
  },
  {
    "platform": "Z1",
    "lead": "Sharmi",
    "title": "Test Proteomics Datasets on pipeline",
    "priority": "P0",
    "size": "Small",
    "description": "The pipeline will be extensively tested on proteomics datasets that are acquired on the Z1",
    "status": "Ongoing",
    "success_criteria": "Proteomics datasets are able to run in an automated manner up to stitching",
    "risks": "Changes in data collection properties, CO upgrades, aind-transfer-service",
    "resource_summary": "Camilo: 10%"
  },
  {
    "platform": "Z1",
    "lead": "Sharmi",
    "title": "Develop AI tools for Proteomics Data Analysis",
    "priority": "P1",
    "size": "Medium/Large",
    "description": "Since acquiring channels is expensive and sometimes challenging due to tissue processing constraints, predicting channels from other channels can greatly boost the abil ability for integrated analysis. In this first step, we will develop an AI based encoder to help develop prediction tools",
    "status": "Ongoing",
    "success_criteria": "A preliminary model that can  encode proteomics data",
    "risks": "OCTO (Geoff), Access to a GPU Cluster for training",
    "resource_summary": "Camilo: 40%"
  },
  {
    "platform": "Behavior Videos",
    "lead": "Sharmi",
    "title": "Release pipeline for video processing with Lightning Pose",
    "priority": "P0",
    "size": "Small",
    "description": "Currently the pipeline exists but needs autotriggering and cleanup before release.",
    "status": "Pretty close to completion",
    "success_criteria": "Successful deployment and release of pipeline \n\nTesting from at least 3 different projects -> auto triggering up to inference, QC done by scientists.",
    "risks": "Some datasets having different metadata and edge cases could add time for getting this finalized",
    "resource_summary": "Jonathan: 10%"
  },
  {
    "platform": "Behavior Videos",
    "lead": "Sharmi",
    "title": "Develop markerless tracking pipeline",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "Multimodal foundation models developed recently can enable us to track animals without the need for annotion. Preliminary tests on behavior videos have shown promising results. A pipeline for testing this on various cases needs to be developed to decide it's applicability for our experimetns",
    "status": "Not started.",
    "success_criteria": "An automated pipeline that takes in a video and text input to indicate body parts of interest and outputs tracks on them.",
    "risks": "Models could be expensive to run, may not generalize well, may need to fine tune them.",
    "resource_summary": "Jonathan: 50%"
  },
  {
    "platform": "Behavior Videos",
    "lead": "Sharmi",
    "title": "Applying 3D Ground Truth Tracking to Multiple Datasets",
    "priority": "P0",
    "size": "Large",
    "description": "A 3d markerless tracking method based on cotracker is being developed by BSA. A training dataset for this needs to be developed. This work involves using gaussian splatting methods for generating this.",
    "status": "Method development is complete. Next steps are scaling to large datasets",
    "success_criteria": "A code base for generating 3D tracks for whole video sequences. Examples on 1-2 datasets.",
    "risks": "Compute costs for full datasets need to be watched.",
    "resource_summary": "Jonathan: 40%"
  },
  {
    "platform": "Behavior Videos",
    "lead": "Sharmi",
    "title": "Generating a Master Dataset for Behavior Video Analysis",
    "priority": "P1",
    "size": "Large",
    "description": "In order to take pose estimation and analysis to the next level we can take advantage of the variety of video data that has been collected at AIND. However, these have all been collected with different conditions and need to be collated to be shared as a collective dataset. This effort which has been started here: will continue to collate datasets.",
    "status": "Started",
    "success_criteria": "An initial dataset shared on aind-benchmark-data",
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "PowerApps and Dataverse Dependent Services (P0, M)",
    "priority": "P0",
    "size": "Medium",
    "description": "A decision has been made to move forward with replacing SLIMS with PowerApps and Dataverse. This will require a couple of additional services to be built to allow PowerApps to communicate with LabTracks and a REST API to be set up to allow users to input and fetch data programmatically.",
    "status": "Not much work has been started in building out the necessary infrastructure.",
    "success_criteria": "\u2022 PowerApps can pull data from LabTracks successfully  \n\u2022 A user can input and fetch waterlog data using a Python API",
    "risks": "We will need Central IT's help in setting up a Remote API Gateway onsite that will allow PowerApps to pull data from aind-metadata-service. Brain Science has built something similar, so we may be able to leverage their expertise.\u00a0\n\nSince we are new to PowerApps, there may be a slight learning curve in setting up a REST API.",
    "resource_summary": "Jon: 10%, Yosef: 5%, Mekhla: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "aind-metadata-mapper for aind-data-schema v2.0 Upgrades (P0, M)",
    "priority": "P0",
    "size": "Medium",
    "description": "The aind-metadata-mapper repo will need to be overhauled to output aind-data-schema v2.0 compliant metadata.",
    "status": "Hasn't been started yet",
    "success_criteria": "All of the currently working packages that build metadata files will need to build aind-data-schema v2.0 compliant metadata (as best as possible). \n\nFIP module makes use of intended measurement endpoint",
    "risks": "The caveat in the Success Criteria is that we should first upgrade the metadata mappers in their current state. There may be some invalid metadata being produced because of missing information, etc. \n\nWe may need other teams contributing to this project also.",
    "resource_summary": "Jon: 5%, Mekhla: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "aind-metadata-service v1.0 Rollout (P0, M)",
    "priority": "P0",
    "size": "Medium",
    "description": "aind-metadata-service is still in v0. We are overhauling the architecture to make it more robust and more performant.",
    "status": "About 75% finished.",
    "success_criteria": "V1.0 is deployed to production.",
    "risks": "We may need to coordinate this with upgrading aind-metadata-service to output aind-data-schema v2.0 files.",
    "resource_summary": "Jon: 5%, Yosef: 5%, Mekhla: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "aind-metadata-service for aind-data-schema 2.0 Upgrades (P0, M)",
    "priority": "P0",
    "size": "Medium",
    "description": "The aind-metadata-service repo will need to be overhauled to output aind-data-schema v2.0 compliant metadata.",
    "status": "Hasn't started",
    "success_criteria": "Users can request v2.0 compliant procedures and subject metadata from our backends.",
    "risks": "We may need to coordinate this with the aind-metadata-service v1.0 rollout.",
    "resource_summary": "Jon: 5%, Mekhla: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "aind-data-transfer-lite (P0, S)",
    "priority": "P1",
    "size": "Small",
    "description": "External collaborators need an easy way to upload data to our AWS buckets",
    "status": "Work has been completed for the core functionality. We are in the testing phase.",
    "success_criteria": "Our collaborators at UPenn have uploaded data \n\n(P2, M) There is a GUI built.",
    "risks": null,
    "resource_summary": "Jon: 5%, Yosef: 5%, Ajar: 30%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "BKP Alignment (P1, L)",
    "priority": "P1",
    "size": "Large",
    "description": "AIND and AIBS would like to align as much as possible on how data is collected and catalogued.",
    "status": "Some initial conversations and planning has been started.",
    "success_criteria": "A decision is made on a common architecture.",
    "risks": null,
    "resource_summary": "Jon: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "AWS SSO (or IAMRA) (P0, M)",
    "priority": "P0",
    "size": "Medium",
    "description": "We are currently using long-term credentials for certain users. This is bad practice and we'd like to switch to authenticating users with a different method. Ideally, SSO would be used. However, we can look at other methods such as IAM Roles Anywhere.",
    "status": "Some exploration on the feasibility has been done.",
    "success_criteria": "All of the long-term credentials handed out to users have been revoked.",
    "risks": "Matt in AIBS might be able to help out.",
    "resource_summary": "Jon: 5%, Yosef: 15%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Observability Stack Deployment (P1, M)",
    "priority": "P1",
    "size": "Medium",
    "description": "We'd like to start using the Grafana dashboards SIPE has set up to view the health of our on-prem services easily.",
    "status": "Hasn't been started yet",
    "success_criteria": "There are dashboards on Grafana that display CPU %, heartbeats, and metrics on our endpoints for aind-metadata-service and aind-data-transfer-service.",
    "risks": "SIPE is migrating Grafana Cloud to an on-prem Grafana Server. We need to wait for that project to be completed.",
    "resource_summary": "Jon: 10%, Yosef: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Auto-Deployments for Cloud and On-Prem Services (P1, M)",
    "priority": "P1",
    "size": "Medium",
    "description": "We are currently manually deploying updates to aind-metadata-service, aind-data-transfer-service, and terraform scripts manually. To mitigate the risks associated with manual updates, we'd like to have a github action build and deploy changes to those services.",
    "status": "40% Done. We are currently testing the auto-deployment of our terraform scripts.",
    "success_criteria": "Our on-prem services update automatically on a github action \n\nOur terraform scripts are automatically deployed",
    "risks": "We may need Central IT's help to allow our github action runners to authenticate to Platform 9.",
    "resource_summary": "Jon: 5%, Yosef: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Data Asset Summary API Endpoint (P0, S)",
    "priority": "P0",
    "size": "Small",
    "description": "We want to have an AI model generate a summary for each data asset in our DocumentDB through a REST API endpoint.",
    "status": "Hasn't been started yet.",
    "success_criteria": "There is an endpoint a user can receive an AI generated summary for a data asset (in production)",
    "risks": "We will need to coordinate with Saskia's team. This depends on the code that generates the summary to be tested and deployed.",
    "resource_summary": "Helen: 10%, Yosef: 5%, Ajar: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "MCP Server Deployment (P1, M)",
    "priority": "P1",
    "size": "Medium",
    "description": "We want an MCP Server Deployed",
    "status": "Hasn't been started yet",
    "success_criteria": "An MCP Server is deployed",
    "risks": "We still need to figure out how to do this.",
    "resource_summary": "Helen: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Vector Embeddings Generation and Deployment (P1, M)",
    "priority": "P1",
    "size": "Medium",
    "description": "We want to automate the vector embeddings generation and update process so that assets are continually vectorized. The vector embeddings also need to be deployed in production once everything is tested.",
    "status": "There is a manual script that currently generates vector embeddings for metadata in DocDB. This allows the metadata chatbot to do vector search on assets in DocDB. Currently, Sreya is exploring different LLM models and embeddings generation processes as well.",
    "success_criteria": "Automated process to generate and update vector embeddings (ideally Lambda function). Vector embeddings also stored in production (ideally alongside the metadata records).",
    "risks": "We have not explored using Lambda functions to generate embeddings. Currently the embeddings script runs for multiple hours. \n\nThis also is dependent on the vectorization logic being finalized (e.g. LLM model, choosing which schemas to vectorize).",
    "resource_summary": "Helen: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Data Access API Support (P1, S)",
    "priority": "P1",
    "size": "Small",
    "description": "Placeholder to indicate that we need to address issues that crop up with our data access services.",
    "status": "Ongoing",
    "success_criteria": "Issues are resolved quickly.",
    "risks": "We have some inkling of tech-debt items that we need to address, but we won't know what future issues might be for the most part.",
    "resource_summary": "Helen: 10%, Ajar: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Analysis Pipeline Support (P1, M)",
    "priority": "P1",
    "size": "Medium",
    "description": null,
    "status": "We are still doing things manually, but we want to write a terraform script to handle requests.",
    "success_criteria": "A Terraform script to auto deploy new analysis pipeline infrastructure",
    "risks": null,
    "resource_summary": "Helen: 5%, Yosef: 5%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Chronic Ephys Data Collection Support (P0, M)",
    "priority": "P0",
    "size": "Medium",
    "description": "There is a project to collect chronic ephys data from many weeks straight. This has required a restructuring of our data infrastructure.",
    "status": "Most of the infrastructure work needed has been finished. We are in the testing phase.",
    "success_criteria": "Chronic Ephys Data Collection is started.",
    "risks": "We have some inkling of tech-debt items that we need to address, but we won't know what future issues might be for the most part.",
    "resource_summary": "Jon: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Horta Cloud and Code Ocean Support (P0, M)",
    "priority": "P0",
    "size": "Medium",
    "description": "We will need to address issues with Horta Cloud and Code Ocean as they crop up.",
    "status": "Ongoing",
    "success_criteria": "Issues are resolved quickly.",
    "risks": "We have some inkling of tech-debt items that we need to address, but we won't know what future issues might be for the most part.",
    "resource_summary": "Yosef: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "aind-qc-portal Support (P1, M)",
    "priority": "P0",
    "size": "Medium",
    "description": "Members of the Data Infrastructure can start contributing to the aind-qc-portal codebase.",
    "status": "The qc-portal is currently running, but there may be room for improving the performance and robustness.",
    "success_criteria": "Complete test coverage. \n\nFigured out reasons for crashes.",
    "risks": "We have some inkling of tech-debt items that we need to address, but we won't know what future issues might be for the most part.",
    "resource_summary": "Jon: 5%, Mekhla: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "aind-data-transfer-service UI (P2, M)",
    "priority": "P1",
    "size": "Medium",
    "description": "We would like to add UI features to make it easier for users to submit jobs through a form. We also want admins to be able to cancel existing jobs.",
    "status": "The data transfer service has a simple UI for users to submit or view jobs. Jobs can be submitted through REST API or attaching a csv or excel sheet.",
    "success_criteria": "Users can submit upload job requests using a form. \n\nAdmins can cancel existing jobs.",
    "risks": "This is not a high priority feature since it is mostly quality-of-life changes.",
    "resource_summary": "Helen: 20%, Ajar: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Data Cleanup and Other Support Requests",
    "priority": null,
    "size": null,
    "description": "We need to set aside time for tasks such as moving data from the private bucket to the open bucket, cleaning up VAST, and other small support requests we get from Scientists. We can also include Scientific Computing Office Hours in this.",
    "status": "Ongoing",
    "success_criteria": "Requests are addressed quickly and efficiently.",
    "risks": "Some requests can be vague",
    "resource_summary": "Jon: 5%, Helen: 5%, Yosef: 5%, Mekhla: 5%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Continuous Learning/Side Projects (P1, S)",
    "priority": "P1",
    "size": "Small",
    "description": "It is important to carve out time for continuous education and side projects. This may include earning AWS certificates, Cornerstone training, seminar attendance, workshops, python training, etc.",
    "status": "Ongoing",
    "success_criteria": "Engineers have dedicated time for this.",
    "risks": "Other things seem to take precedence over this and only 5% of time is spent.",
    "resource_summary": "Jon: 10%, Helen: 10%, Yosef: 10%, Mekhla: 10%, Ajar: 10%"
  }
]