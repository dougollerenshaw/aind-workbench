[
  {
    "platform": "Behavior",
    "lead": "Arielle",
    "title": "Combine existing VR Foraging capsules into a full production pipeline",
    "priority": "P0",
    "size": "small",
    "description": "Combine the raw and processing VR Foraging NWB packaging capsules into a pipeline which will output a final NWB plus relevant metadata.  ",
    "status": "Not started. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 5%"
  },
  {
    "platform": "Behavior",
    "lead": "Arielle",
    "title": "Create behavior pipeline template",
    "priority": "P2",
    "size": "small",
    "description": "We plan to build a generic template of a processing pipeline to help users to build their own pipeline. This should help reduce reliance on scientific computing for future behavior projects. ",
    "status": "Not started. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 5%"
  },
  {
    "platform": "Behavior",
    "lead": "Arielle",
    "title": "Upgrade metadata to schema 2.0",
    "priority": "P0",
    "size": "medium",
    "description": "\u2022 Identify all instruments using physio-behavior processing pipelines \n\u2022 Update pipelines to be backwards compatible with 1.x and 2.0 schema versions \n\u2022 Update metadata mapper code \n\u2022 Coordinate with users to get code deployed on rigs (make an excel sheet to track what's been done and what's been deployed) \n\u2022 Do we need to help with rig.jsons?",
    "status": "Not started. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 15%, Ahad: 15%"
  },
  {
    "platform": "Behavior",
    "lead": "Arielle",
    "title": "BCI behavior wrap-up",
    "priority": "P0",
    "size": "medium",
    "description": "Finish work related to the BCI behavior task. We made substantial progress in cleaning up the data processing pipeline during preparations for the May hackathon at UW. It\u2019s important to maintain this momentum to ensure that these efforts are translated into a robust processing pipeline. ",
    "status": "This is a continuation of work begun in the leadup to the May UW Hackathon event. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 20%"
  },
  {
    "platform": "Behavior",
    "lead": "Arielle",
    "title": "Ongoing support (solving github issues)",
    "priority": "P1",
    "size": "medium",
    "description": "Provide ongoing support for existing pipelines. This involves triaging and solving issues submitted by scientists through Github. ",
    "status": "Not started. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 20%"
  },
  {
    "platform": "Analysis",
    "lead": "Arielle",
    "title": "Scaled analysis",
    "priority": "P0",
    "size": "small",
    "description": "The analysis pipeline needs to support multiple analysis per derived asset. We will scale from N jobs (current system) to N x M jobs, where N = derived asset and M = analysis type ",
    "status": "Job dispatch and analysis wrapper templates exist as well as a pipeline but do not contain instructions on how to work with the pipeline. The goal is to create a low-friction user experience for running scaled, versioned analysis. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 6%"
  },
  {
    "platform": "Analysis",
    "lead": "Arielle",
    "title": "Add nested outputs to job-dispatch",
    "priority": "P0",
    "size": "small",
    "description": "Users need the ability to pass nested, or grouped, asset ids to their analysis wrapper. For example, a query should output groups of assets that can be passed to a given analysis capsule.  ",
    "status": "Does not exist. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 6%"
  },
  {
    "platform": "Analysis",
    "lead": "Arielle",
    "title": "Viewing application of analysis results",
    "priority": "P1",
    "size": "medium",
    "description": "Users need to check the results of their analysis after running the analysis pipeline. This can be achieved by building a Panel application that can query analysis location information from analysis collections in the document database.\u00a0",
    "status": "Does not exist. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 20%"
  },
  {
    "platform": "Analysis",
    "lead": "Arielle",
    "title": "Documentation and templates",
    "priority": "P0",
    "size": "medium",
    "description": "Publish documentation with clear examples on how to use the analysis pipeline. The goal is to create a template pipeline with clear instructions that would be frictionless for any non-power user to replicate.\u00a0",
    "status": "Working on finalizing templates for both the job dispatch and analysis wrapper",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arielle: 10%"
  },
  {
    "platform": "Analysis",
    "lead": "Arielle",
    "title": "Continuous analysis pipeline ",
    "priority": "P1",
    "size": "medium",
    "description": "For analysis that needs to be run on all derived assets output from a processing pipeline, a system is required that can check for new assets as they are processed. The trigger system will need access to the results collection so that it can check if analysis has already been run on a given asset. ",
    "status": "Does not exist. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 10%"
  },
  {
    "platform": "Fiber Photometry",
    "lead": "Arielle",
    "title": "Backlog re-processing",
    "priority": "P0",
    "size": "tiny",
    "description": "Reprocess backlog tickets. ",
    "status": "Not started ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 2%"
  },
  {
    "platform": "Fiber Photometry",
    "lead": "Arielle",
    "title": "Upgrade metadata to schema 2.0",
    "priority": "P0",
    "size": "medium",
    "description": "\u2022 Identify all instruments using physio-behavior processing pipelines \n\u2022 Update pipelines to be backwards compatible with 1.x and 2.0 schema versions \n\u2022 Update metadata mapper code \n\u2022 Coordinate with users to get code deployed on rigs (make an excel sheet to track what's been done and what's been deployed) \n\u2022 Do we need to help with rig.jsons?",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arjun: 15%, Ahad: 15%"
  },
  {
    "platform": "Fiber Photometry",
    "lead": "Arielle",
    "title": "Convert rigs to upload using modular pipelines",
    "priority": "P1",
    "size": "medium",
    "description": "Coordinate with users to submit jobs to the aind-data-transfer service V2 and point to the fiber specific job - type that will process through both fiber and behavior pipelines",
    "status": "Not started ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 20%"
  },
  {
    "platform": "Fiber Photometry",
    "lead": "Arielle",
    "title": "Ongoing support (solving github issues)",
    "priority": "P1",
    "size": "medium",
    "description": "Provide ongoing support for existing pipelines. This involves triaging and solving issues submitted by scientists through Github. ",
    "status": "Not started. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Ahad: 20%"
  },
  {
    "platform": "Dataverse - PowerApps",
    "lead": "Arielle",
    "title": "Two-way integration and restriction tracking",
    "priority": "P0",
    "size": "medium",
    "description": "Waterlog can read and write to Dataverse. More tables will need to be added for state changes (water on-off) for mice in active behavior. ",
    "status": "Not done. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Clark: 10%"
  },
  {
    "platform": "Dataverse - PowerApps",
    "lead": "Arielle",
    "title": "Behavior protocol integration and automation ",
    "priority": "P1",
    "size": "medium",
    "description": "Application viewer for users to determine where the mouse is in its behavior protocol. Model full behavioral training progression and enable automated transitions and calculations as extra validation for water restricting mice. ",
    "status": "Not done. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Clark: 30%"
  },
  {
    "platform": "Dataverse - PowerApps",
    "lead": "Arielle",
    "title": "Inventory system",
    "priority": "P1",
    "size": "Medium/Large",
    "description": "Create types of locations (freezer, bench top, etc.), types of inventories (reagent, dyes, viruses, etc.) and their supporting information, status of items (empty, full, etc.) and add logic around how it shows up in inventory. Export data in SLIMS and upload to Dataverse. Requires barcoding, build flows and create forms for users to view / update items in system. ",
    "status": "Not done. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Clark: 40%"
  },
  {
    "platform": "Dataverse - PowerApps",
    "lead": "Arielle",
    "title": "Ongoing support for transition from SLIMS to Dataverse",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "Supporting users in SLIMS while the transition is ongoing into Dataverse. This includes but is not limited to exporting data from SLIMS to Dataverse. Maintaining current workflows and remap data to the Dataverse system. ",
    "status": "Not done. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Clark: 40%"
  },
  {
    "platform": "2P Ophys",
    "lead": "Arielle",
    "title": "Create ISI metadata and ISI capsule",
    "priority": "P0",
    "size": "small",
    "description": "David, Di will build the ISI capsule and Arielle will attach metadata to it",
    "status": "Finished",
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "2P Ophys",
    "lead": "Arielle",
    "title": "Test and point rigs to upload to the new pipeline",
    "priority": "P0",
    "size": "medium",
    "description": "Coordinate with users to submit jobs to the aind-data-transfer service V2 and point to the fiber specific job - type that will process through both ophys and behavior pipelines",
    "status": "Not started",
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "2P Ophys",
    "lead": "Arielle",
    "title": "Data and metadata cleanup",
    "priority": "P0",
    "size": "medium",
    "description": "Fix derived data descriptions, session metadata and any other metadata that needs to be updated. Remove duplicate sessions. ",
    "status": "Not started",
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "2P Ophys",
    "lead": "Arielle",
    "title": "Ongoing support (supporting releases and summer workshop)",
    "priority": "P0",
    "size": "medium",
    "description": "Provide ongoing support for existing pipelines. This involves triaging and solving issues submitted by scientists through Github. ",
    "status": "Not started",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Arielle: 20%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Manuscript on Chatbot",
    "priority": "P0",
    "size": "Large",
    "description": "Write a manuscript on the metadata chatbot that describes the tools and the benchmarking results. ",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Saskia: 10%, Sreya: 70%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Continued development of Chatbot",
    "priority": "P1",
    "size": "Medium",
    "description": "Continue to improve chatbot and mcp performance ",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Sreya: 30%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "AIND Data Schema v2.0 deployment",
    "priority": "P0",
    "size": "Large",
    "description": "V2.0 is being released this week. The upgrader is working on upgrading existing 1.0 records, and identifying migration issues in its wake.  ",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Doug: 10%, Dan: 80%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Analysis Architecture",
    "priority": "P0",
    "size": "Medium",
    "description": null,
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Support external users",
    "priority": "P1",
    "size": "Medium",
    "description": "Help Templeton collaborators get data and metadata into the platform",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "LC Paper publication",
    "priority": "P0",
    "size": "Large",
    "description": "v2.0 is being released this week. The upgrader is working on upgrading existing 1.0 records, and identifying migration issues in its wake.  ",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Doug: 60%, Tom: 20%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Data Challenge",
    "priority": "P0",
    "size": "Medium",
    "description": "Preparation for a data challenge on image compression to be held in early 2026. Kaggle research application is due August 11th.  ",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Suyee: 30%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Move Dynamic Routing data into the public bucket",
    "priority": "P0",
    "size": "Large",
    "description": "Much of our data is still being put into private buckets. We are working to move assets into the open bucket in compliance with our policy and to facilitate making public collections. This involves fixing a lot of missing metadata, so it can\u2019t be completed in one shot. We have created a dashboard to track data in different buckets according to project. ",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Doug: 30%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Other summer workshops",
    "priority": "P1",
    "size": "Small",
    "description": "Participating in TReND (remotely), the Coding Workshop, the Technical Workshop. ",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Saskia: 10%, Suyee: 10%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Fall workshop planning",
    "priority": "P1",
    "size": "Small",
    "description": "We are planning a UW CNC hackathon in the fall as well as a workshop at WWU. These will be in Q4, but planning will need to happen before then. The WWU should largely leverage existing content. The UW event will likely involve some preparation for hackathon notebooks, etc. Ideally this will leverage content from SWDB. ",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Suyee: 20%"
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "protocol and website",
    "priority": "P2",
    "size": "Small",
    "description": null,
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Data & Outreach",
    "lead": "Saskia",
    "title": "Platform Paper ",
    "priority": "P1",
    "size": "Large",
    "description": "Write a manuscript on the data platform strategy that highlights our infrastructure and philosophy.  ",
    "status": null,
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Saskia: 30%"
  },
  {
    "platform": "Ephys",
    "lead": "David",
    "title": "Chronic Spike Sorting",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "Develop sorting workflow that is compatible with chronic recordings",
    "status": "Not started",
    "success_criteria": "Depends on DartSort developers delivering DartSort.",
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Ephys",
    "lead": "David",
    "title": "Ephys pipeline manuscript",
    "priority": "P0",
    "size": "Large",
    "description": "Write and submit a manuscript describing the ephys pipeline.",
    "status": "In development",
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Ephys",
    "lead": "David",
    "title": "SpikeInterface GUI",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "Fully integrate SpikeInterface GUI into QC portal",
    "status": "In development",
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Slap2",
    "lead": "David",
    "title": "Implement SLAP2 processing pipeline",
    "priority": "P1",
    "size": "Medium/Large",
    "description": "Implement SLAP2 pipeline based on evaluated parameters.",
    "status": "Not started ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "SmartSPIM",
    "lead": "Sharmi",
    "title": "Upgrade SmartSPIM Pipeline to incorporate new spot detection and new classification model",
    "priority": "P0",
    "size": "medium/large",
    "description": "A new spot detection and classification model has been developed and needs to be QC'ed and deployed into the pipeline",
    "status": "Currently blocked by our ability to run on the Slurm Cluster where we are testing the new detection model",
    "success_criteria": "Scientists for QC, CO",
    "risks": "CO",
    "resource_summary": "Camilo: 20%"
  },
  {
    "platform": "SmartSPIM",
    "lead": "Sharmi",
    "title": "Test External DataSet on Slurm Cluster",
    "priority": "P1",
    "size": "small/medium",
    "description": "UW submitted a brain to us and we need to run it through our pipeline to show it's impact for external labs (and our paper)",
    "status": "Partially run",
    "success_criteria": "QC from UW, slurm cluster, updates needed for adapting to differences in the data",
    "risks": "Slurm Cluster",
    "resource_summary": "Camilo: 15%"
  },
  {
    "platform": "ExaSPIM",
    "lead": "Sharmi",
    "title": "Setup pipeline for CCF Registration, Soma detection and quantification",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "The different capsules have been setup for various processing, but we need to do some infrastructure work to connect them and ensure seamless outputs",
    "status": "Each piece is complete but lots of inconsistency in data outputs",
    "success_criteria": "This will be setup as two pipelines since we are still debugging the latest bigstitcher for fusing the signal channel. Upgrades to capsules can delay this",
    "risks": "EMR Cluster, CO",
    "resource_summary": "Di: 40%"
  },
  {
    "platform": "ExaSPIM",
    "lead": "Sharmi",
    "title": "Develop metrics for comparing across alignment results",
    "priority": "p0",
    "size": "Large",
    "description": "Currently we are unable to compare registration of the same interest points when the alignment changes since bigstitcher only outputs collated summary statistics. We will with with the Octo (Rhapso Team) to develop metrics that will enable this. ",
    "status": "Not started. ",
    "success_criteria": "Changes to bigstitcher outputs and performance can delay this, A QC loop with scientists is also needed",
    "risks": null,
    "resource_summary": "Di: 50%"
  },
  {
    "platform": "ExaSPIM",
    "lead": "Sharmi",
    "title": "Image compression",
    "priority": "p0",
    "size": "Large",
    "description": "Currently we compress with standard zstd libraries. However, we can optimize more for our data. This work is an exploration of the different methods for compression.",
    "status": "Ongoing",
    "success_criteria": null,
    "risks": "CO, S3 storate",
    "resource_summary": null
  },
  {
    "platform": "Z1",
    "lead": "Sharmi",
    "title": "Determine an optimal method for computing R2R registration",
    "priority": "P0",
    "size": "Large",
    "description": "R2R registration with bigstream has proved to be too expensive to deploy for routine data processing. An alternative method needs to be developed.",
    "status": "Tests with Bigstitcher are promising",
    "success_criteria": "Bigstitcher, Bigstream",
    "risks": "CO",
    "resource_summary": "Sean: 100%"
  },
  {
    "platform": "Z1",
    "lead": "Sharmi",
    "title": "Test Proteomics Datasets on pipeline",
    "priority": "P0",
    "size": "Small",
    "description": "The pipeline will be extensively tested on proteomics datasets that are acquired on the Z1",
    "status": "Ongoing",
    "success_criteria": "Changes in data collection properties, CO upgrades, aind-transfer-service",
    "risks": "Slurm Cluster, CO",
    "resource_summary": "Camilo: 10%"
  },
  {
    "platform": "Z1",
    "lead": "Sharmi",
    "title": "Develop AI tools for Proteomics Data Analysis",
    "priority": "P1",
    "size": "Medium/Large",
    "description": "Since acquiring channels is expensive and sometimes challenging due to tissue processing constraints, predicting channels from other channels can greatly boost the abil ability for integrated analysis. In this first step, we will develop an AI based encoder to help develop prediction tools",
    "status": "Ongoing",
    "success_criteria": "OCTO (Geoff), Access to a GPU Cluster for training",
    "risks": "GPU Cluster",
    "resource_summary": "Camilo: 40%"
  },
  {
    "platform": "Behavior Videos",
    "lead": "Sharmi",
    "title": "Develop markerless tracking pipeline",
    "priority": "P0",
    "size": "Medium/Large",
    "description": "Multimodal foundation models developed recently can enable us to track animals without the need for annotion. Preliminary tests on behavior videos have shown promising results. A pipeline for testing this on various cases needs to be developed to decide it's applicability for our experimetns",
    "status": "Not started. ",
    "success_criteria": "Models could be expensive to run, may not generalize well, may need to fine tune them.",
    "risks": "CO",
    "resource_summary": "Jonathan: 50%"
  },
  {
    "platform": "Behavior Videos",
    "lead": "Sharmi",
    "title": "Applying 3D Ground Truth Tracking to Multiple Datasets ",
    "priority": "P0",
    "size": "Large",
    "description": "A 3d markerless tracking method based on cotracker is being developed by BSA. A training dataset for this needs to be developed. This work involves using gaussian splatting methods for generating this.",
    "status": "Method development is complete. Next steps are scaling to large datasets",
    "success_criteria": "Compute costs for full datasets need to be watched.",
    "risks": "CO",
    "resource_summary": "Jonathan: 40%"
  },
  {
    "platform": "Behavior Videos",
    "lead": "Sharmi",
    "title": "Generating a Master Dataset for Behavior Video Analysis ",
    "priority": "P1",
    "size": "Large",
    "description": "In order to take pose estimation and analysis to the next level we can take advantage of the variety of video data that has been collected at AIND. However, these have all been collected with different conditions and need to be collated to be shared as a collective dataset. This effort which has been started here: will continue to collate datasets. ",
    "status": "Started",
    "success_criteria": null,
    "risks": null,
    "resource_summary": null
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "aind-metadata-mapper for aind-data-schema v2.0 Upgrades (P0, M) ",
    "priority": "P0",
    "size": "Medium",
    "description": "The aind-metadata-mapper repo will need to be overhauled to output aind-data-schema v2.0 compliant metadata. ",
    "status": "Hasn't been started yet ",
    "success_criteria": "The caveat in the Success Criteria is that we should first upgrade the metadata mappers in their current state. There may be some invalid metadata being produced because of missing information, etc. \n\nWe may need other teams contributing to this project also. ",
    "risks": null,
    "resource_summary": "Jon: 5%, Mekhla: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "aind-metadata-service v1.0 Rollout (P0, M) ",
    "priority": "P0",
    "size": "Medium",
    "description": "aind-metadata-service is still in v0. We are overhauling the architecture to make it more robust and more performant. ",
    "status": "About 75% finished. ",
    "success_criteria": "We may need to coordinate this with upgrading aind-metadata-service to output aind-data-schema v2.0 files. ",
    "risks": "No additional",
    "resource_summary": "Jon: 5%, Yosef: 5%, Mekhla: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "aind-metadata-service for aind-data-schema 2.0 Upgrades (P0, M) ",
    "priority": "P0",
    "size": "Medium",
    "description": "The aind-metadata-service repo will need to be overhauled to output aind-data-schema v2.0 compliant metadata. ",
    "status": "Hasn't started ",
    "success_criteria": "We may need to coordinate this with the aind-metadata-service v1.0 rollout. ",
    "risks": null,
    "resource_summary": "Jon: 5%, Mekhla: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "aind-data-transfer-lite (P0, S) ",
    "priority": "P1",
    "size": "Small ",
    "description": "External collaborators need an easy way to upload data to our AWS buckets ",
    "status": "Work has been completed for the core functionality. We are in the testing phase. ",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Jon: 5%, Yosef: 5%, Ajar: 30%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "BKP Alignment (P1, L) ",
    "priority": "P1",
    "size": "Large",
    "description": "AIND and AIBS would like to align as much as possible on how data is collected and catalogued. ",
    "status": "Some initial conversations and planning has been started. ",
    "success_criteria": null,
    "risks": "TBD",
    "resource_summary": "Jon: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "AWS SSO (or IAMRA) (P0, M) ",
    "priority": "P0",
    "size": "Medium",
    "description": "We are currently using long-term credentials for certain users. This is bad practice and we'd like to switch to authenticating users with a different method. Ideally, SSO would be used. However, we can look at other methods such as IAM Roles Anywhere. ",
    "status": "Some exploration on the feasibility has been done. ",
    "success_criteria": "Matt in AIBS might be able to help out.\t ",
    "risks": null,
    "resource_summary": "Jon: 5%, Yosef: 15%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Observability Stack Deployment (P1, M) ",
    "priority": "P1",
    "size": "Medium",
    "description": "We'd like to start using the Grafana dashboards SIPE has set up to view the health of our on-prem services easily. ",
    "status": "Hasn't been started yet ",
    "success_criteria": "SIPE is migrating Grafana Cloud to an on-prem Grafana Server. We need to wait for that project to be completed. ",
    "risks": null,
    "resource_summary": "Jon: 10%, Yosef: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Auto-Deployments for Cloud and On-Prem Services (P1, M) ",
    "priority": "P1",
    "size": "Medium",
    "description": "We are currently manually deploying updates to aind-metadata-service, aind-data-transfer-service, and terraform scripts manually. To mitigate the risks associated with manual updates, we'd like to have a github action build and deploy changes to those services. ",
    "status": "40% Done. We are currently testing the auto-deployment of our terraform scripts. ",
    "success_criteria": "We may need Central IT's help to allow our github action runners to authenticate to Platform 9. ",
    "risks": "Minimal. We may need self-hosted github action runners, but they won't require a lot of compute resources. ",
    "resource_summary": "Jon: 5%, Yosef: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Data Asset Summary API Endpoint (P0, S) ",
    "priority": "P0",
    "size": "Small ",
    "description": "We want to have an AI model generate a summary for each data asset in our DocumentDB through a REST API endpoint. ",
    "status": "Hasn't been started yet. ",
    "success_criteria": "We will need to coordinate with Saskia's team. This depends on the code that generates the summary to be tested and deployed. ",
    "risks": "Still TDB. Ideally, we'd like to set this up using AWS Lambda, but it depends on the timeout and memory required. ",
    "resource_summary": "Helen: 10%, Yosef: 5%, Ajar: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "MCP Server Deployment (P1, M) ",
    "priority": "P1",
    "size": "Medium",
    "description": "We want an MCP Server Deployed ",
    "status": "Hasn't been started yet ",
    "success_criteria": "We still need to figure out how to do this. ",
    "risks": "TBD",
    "resource_summary": "Helen: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Vector Embeddings Generation and Deployment (P1, M) ",
    "priority": "P1",
    "size": "Medium",
    "description": "We want to automate the vector embeddings generation and update process so that assets are continually vectorized. The vector embeddings also need to be deployed in production once everything is tested. ",
    "status": "There is a manual script that currently generates vector embeddings for metadata in DocDB. This allows the metadata chatbot to do vector search on assets in DocDB. Currently, Sreya is exploring different LLM models and embeddings generation processes as well. ",
    "success_criteria": "We have not explored using Lambda functions to generate embeddings. Currently the embeddings script runs for multiple hours. \n\nThis also is dependent on the vectorization logic being finalized (e.g. LLM model, choosing which schemas to vectorize). ",
    "risks": "TBD",
    "resource_summary": "Helen: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Data Access API Support (P1, S) ",
    "priority": "P1",
    "size": "Small ",
    "description": "Placeholder to indicate that we need to address issues that crop up with our data access services. ",
    "status": "Ongoing",
    "success_criteria": "We have some inkling of tech-debt items that we need to address, but we won't know what future issues might be for the most part. ",
    "risks": null,
    "resource_summary": "Helen: 10%, Ajar: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Analysis Pipeline Support (P1, M) ",
    "priority": "P1",
    "size": "Medium",
    "description": null,
    "status": "We are still doing things manually, but we want to write a terraform script to handle requests.",
    "success_criteria": null,
    "risks": null,
    "resource_summary": "Helen: 5%, Yosef: 5%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Chronic Ephys Data Collection Support (P0, M) ",
    "priority": "P0",
    "size": "Medium",
    "description": "There is a project to collect chronic ephys data from many weeks straight. This has required a restructuring of our data infrastructure. ",
    "status": "Most of the infrastructure work needed has been finished. We are in the testing phase. ",
    "success_criteria": "We have some inkling of tech-debt items that we need to address, but we won't know what future issues might be for the most part. ",
    "risks": "We plan to upload data in chunks, and each chunk shouldn't require much compute resources. ",
    "resource_summary": "Jon: 10%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Horta Cloud and Code Ocean Support (P0, M) ",
    "priority": "P0",
    "size": "Medium",
    "description": "We will need to address issues with Horta Cloud and Code Ocean as they crop up. ",
    "status": "Ongoing ",
    "success_criteria": "We have some inkling of tech-debt items that we need to address, but we won't know what future issues might be for the most part. ",
    "risks": null,
    "resource_summary": "Yosef: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "aind-qc-portal Support (P1, M) ",
    "priority": "P0",
    "size": "Medium",
    "description": "Members of the Data Infrastructure can start contributing to the aind-qc-portal codebase. ",
    "status": "The qc-portal is currently running, but there may be room for improving the performance and robustness. ",
    "success_criteria": "We have some inkling of tech-debt items that we need to address, but we won't know what future issues might be for the most part. ",
    "risks": "Little to None. ",
    "resource_summary": "Jon: 5%, Mekhla: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "aind-data-transfer-service UI (P2, M) ",
    "priority": "P1",
    "size": "Medium",
    "description": "We would like to add UI features to make it easier for users to submit jobs through a form. We also want admins to be able to cancel existing jobs. ",
    "status": "The data transfer service has a simple UI for users to submit or view jobs. Jobs can be submitted through REST API or attaching a csv or excel sheet. ",
    "success_criteria": "This is not a high priority feature since it is mostly quality-of-life changes. ",
    "risks": "Little to None. ",
    "resource_summary": "Helen: 20%, Ajar: 20%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Data Cleanup and Other Support Requests ",
    "priority": null,
    "size": null,
    "description": "We need to set aside time for tasks such as moving data from the private bucket to the open bucket, cleaning up VAST, and other small support requests we get from Scientists. We can also include Scientific Computing Office Hours in this. ",
    "status": "Ongoing ",
    "success_criteria": "\tSome requests can be vague ",
    "risks": "Minimal. ",
    "resource_summary": "Jon: 5%, Helen: 5%, Yosef: 5%, Mekhla: 5%"
  },
  {
    "platform": "Data Infrastructure",
    "lead": "Jon",
    "title": "Continuous Learning/Side Projects (P1, S) ",
    "priority": "P1",
    "size": "Small ",
    "description": "It is important to carve out time for continuous education and side projects. This may include earning AWS certificates, Cornerstone training, seminar attendance, workshops, python training, etc. ",
    "status": "Ongoing ",
    "success_criteria": "Other things seem to take precedence over this and only 5% of time is spent.",
    "risks": "Minimal",
    "resource_summary": "Jon: 10%, Helen: 10%, Yosef: 10%, Mekhla: 10%, Ajar: 10%"
  }
]